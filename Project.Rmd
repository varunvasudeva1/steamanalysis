---
title: "STA 483 Final Project"
author: "Varun Vasudeva, Simon Louisin, Sean Finnigan, Gina Krynski"
date: "4/26/2022"
output: html_document
---

```{r warning=FALSE, message=FALSE}
library(tidyverse)
library(forecast)
library(TSA)
library(tseries)
library(fpp2)
library(lubridate)
```

This report is comprised of comprehensive analyses for four games over the period Jan 2012 - Oct 2021. We are using monthly player data from Steam, a game distribution platform, to understand how these games from different genres have changed in popularity over time and if any conclusions can be drawn about the player engagement trends of genres based on the results we obtain.

## Game 1: Universe Sandbox

### Game Description & Data Processing

Universe Sandbox is a physics-based space simulator. It merges gravity, climate, collision, and material interactions to reveal the beauty of our universe and the fragility of our planet.

```{r}
steam <- read.csv('AllSteamData.csv')
steam <- steam %>%
  drop_na() %>%
  filter(Name == 'Universe Sandbox', Month != 'Last 30 Days')
head(steam)
```

```{r}
steam.ts <- ts(steam$Avg..Players, start=c(2013, 1), end=c(2021, 6), frequency=12)
autoplot(steam.ts)
```

### ACF and PACF Plots

```{r}
ggAcf(steam.ts)
ggPacf(steam.ts)
```

The ACF output shows us an ACF plot very reminiscent of the AR(1) process's ACF plot. This will be helpful in model building, as we will see later on. The ACF shows all lags till lag 24 as having significant autocorrelation. The PACF further reveals that most of the autocorrelation at any given lag is due to the autocorrelation of the first lag.

### Stationarity Analysis

We can now examine the data to see if differencing induces stationarity.

```{r}
steam.ts %>%
  diff() %>%
  ggtsdisplay()
```

Taking the first difference of the data indeed makes it mostly stationary. We can see this by the `autoplot()` result in the top frame showing the monthly players over time centered at 0 and having relatively constant variance throughout the plot. The negative spike at the start of 2019 makes the constant variance assumption come under scrutiny but it can be attributed to randomness since there is only one of them (this indicates it isn't a systemic issue).

Additionally, the ACF plot shows a significant ACF spike at lag 1, but it is within the significance boundary denoted by the dotted blue line. The same is true for the PACF, where the first lag is responsible for most of the significant ACF when removing the effect of the other lags. There is no significant lag in the PACF apart from the first one, until perhaps the 18th lag (but this is not considered).

### Seasonality Analysis

```{r}
ggmonthplot(steam.ts)
```

`ggmonthplot()` shows us a somewhat sinusoidal trend in the average values of monthly players by month across 2013-2021. However, judging the black line plots behind the horizontal, blue, mean-value markers more closely, we can see there is extreme variability in this data, making the mean a bad choice for evaluating the seasonality of this data.

```{r}
ggseasonplot(steam.ts)
```

The `ggseasonplot()` output shows us that the trend for most years looks very much like white-noise, with random oscillations. The 2019, 2020, and 2021 years are the only ones that deviate from this trend, making them aberrant years. We can explain this steep decline in monthly players by the release of the sequel to this game, Universe Sandbox 2.

```{r}
p <- periodogram(steam.ts)
maxidx <- which(p$spec == max(p$spec))
p$freq[maxidx]
```

There is a large periodogram spike at $\omega=0.009259259\approx0.01$. However, this is not representative of any real usable statistic - for example, a bimonthly or semi-annual spike in monthly average users. Thus, we can ignore this and take it as further proof that the data is not seasonal in nature.

### Model Building

We know to include a difference of 1 in our models because we found our data to be stationary when differenced once. Thus, we will start building models with d = 1 within our ARIMA parameters. Going back to Stationarity Analysis, the ACF and PACF plots we obtained upon taking the first difference were strikingly similar to an MA(1) model.

```{r}
mod1 <- arima(steam.ts, order=c(0, 1, 1))
checkresiduals(mod1)
```

This model is a great fit for the data. We can tell by the extremely high p-value of 0.9602. This indicates we can fail to reject the null hypothesis that there is a lack of fit, and by a significant margin at that, since $p=0.96 \gg 0.05$. We can also tell since the ACF and PACF plots show no significant lags at all, as seen by all ACF values being between the dotted blue lines demarcating significance.

Another model worth trying immediately is ARIMA(1, 0, 0). We can tell because the initial ACF and PACF plots for `steam.ts` before differencing revealed similarities with the ACF and PACF plots of the AR(1) process.

```{r}
mod2 <- arima(steam.ts, order=c(1, 0, 0))
checkresiduals(mod2)
```

This model also fits the data very well. It has a high p-value of 0.8781 and shows no significant lags either. However, the ARIMA(0, 1, 1) model has a higher p-value and has no significant ACF at spike 1. Thus, so far, the ARIMA(0, 1, 1) model provides a preferred fit and adds to the complexity of ARIMA(1, 0, 0) by only one parameter.

```{r}
mod3 <- arima(steam.ts, order=c(2, 0, 0))
checkresiduals(mod3)
```

We see that incrementing the AR parameter by 1 results in a further boost in predictive performance. This makes sense as the PACF plot for the non-differenced data showed the biggest ACF spike at lag 1 but also a smaller, less significant but still non-zero ACF at lag 2. Accounting for this second, smaller lag would result in a better model fit, as we're seeing above.

Before selecting a model, we can also look at the selection `auto.arima()` comes to, to see if there are unexplored parameter configurations that result in better predictive performance.

```{r}
automod <- auto.arima(steam.ts)
checkresiduals(automod)
```

The automatic selection defaulted to ARIMA(0, 1, 1) as the ideal model. We will now compare AICs to come to a final decision between ARIMA(0, 1, 1), ARIMA(1, 0, 0), and ARIMA(2, 0, 0).

```{r}
data.frame("Model" = c('ARIMA(0,1,1)', 'ARIMA(1,0,0)', 'ARIMA(2,0,0)'),
           "AIC" = c(mod1$aic, mod2$aic, mod3$aic),
           "S2" = c(mod1$sigma2, mod2$sigma2, mod3$sigma2))
```

The table results show us that ARIMA(1, 0, 0) has the highest AIC and $\sigma^2$ value, so it is the least optimal model and can be discarded. Now the choice is between ARIMA(0, 1, 1) and ARIMA(2, 0, 0). Although ARIMA(2, 0, 0) has the same number of parameters and very, very similar performance to the ARIMA(0, 1, 1), we agree with the `auto.arima()` result because

1) AIC of ARIMA(0, 1, 1) < AIC of ARIMA(2, 0, 0), and,

2) even though $\sigma^2$ of ARIMA(2, 0, 0) < $\sigma^2$ of ARIMA(0, 1, 1), the difference in $\sigma^2$ values (= 4) is much smaller than the difference in AIC (= 16), making AIC our priority selection criteria.

```{r}
mod1
```

Thus, our final model is ARIMA(0, 1, 1), whose model equation is
\[
  (1-B)Y_{t}=\varepsilon_{t}-0.2\varepsilon_{t-1}
\]






## Game 2: Stardew Valley

### Game Description & Data Processing

### ACF and PACF Plots

### Stationarity Analysis

### Seasonality Analysis

### Model Building






## Game 3: Counter-Strike: Global Offensive

### Game Description & Data Processing

Counter Strike: Global Offensive, commonly nicknamed as CSGO, is a first person shooter with online multiplayer. The data below works with the games average monthly plays for 2012-2021. 

```{r, warning=FALSE}
data <- read.csv("AllSteamData.csv")

# head(data)
# colnames(data)

CSGOdata <- data[data$Name == "Counter-Strike: Global Offensive",] %>%
  select("Avg..Players","Month") %>%
  mutate(Month = my(Month)) %>%
  remove_missing() 
CSGOdata <- CSGOdata[order(CSGOdata$Month),]

csgo.ts <- ts(CSGOdata$Avg..Players, start = c(2012,7), end =c(2021, 9), frequency = 12)

```

### Initial Analyzation

```{r}
autoplot(csgo.ts) + labs(y = "Avg Monthly Players", title="CSGO Average Players per Month")

Box.test(csgo.ts, type = "Ljung-Box")
adf.test(csgo.ts)

```

The initial plot does not suggest it is a stationary process, as the mean does not appear constant and the variance does not look relatively constant over time. The test statistic of the dickey fuller also indicate that the data is not stationary.  



### Stationarity Analysis and ACF PACF Plots

```{r}
## First differnece test 
csgod1 <- csgo.ts %>% diff(lag=1)
autoplot(csgod1) + 
  labs(y = "Avg Monthly Players", title="CSGO Average Players per Month") + 
  scale_y_continuous(labels = scales::comma)

ggtsdisplay(csgod1)

ggtsdisplay(log(csgod1))
adf.test(csgod1)
```

First difference seems to help stationary. Looking at the ACF PACF, seems to suggest MA(1) or MA(2) because the ACF has spike then drops in a frequent pattern. The PACF has similar plot but Spike at 1 and 2. I will try modeling the data with both and compare the fits.

### Seasonality Analysis

```{r}
ggmonthplot(csgo.ts) + labs(y = "Avg Monthly Players")

ggseasonplot(csgo.ts) + labs(y = "Avg Monthly Players")


```

There doesn't appear to have any obvious seasonality or patterns looking at these plots and the over all plot of the data.  

### Model Building

```{r}
MA1.csgo <- Arima(csgo.ts, order=c(0,1,1))
MA1.csgo

MA2.csgo <- Arima(csgo.ts, order = c(0,1,2))
MA2.csgo

plot(MA1.csgo$x, col="red") 
lines(fitted(MA1.csgo), col="blue")

plot(MA2.csgo$x, col="red") 
lines(fitted(MA2.csgo), col="blue")

results <- data.frame(Fit = c("AIC", "AICc","BIC"),
                      MA_1 =c(MA1.csgo$aic, MA1.csgo$aicc, MA1.csgo$bic),
                      MA_2 = c(MA2.csgo$aic, MA2.csgo$aicc, MA2.csgo$bic))
results
```

The output for Arima(0,1,1) labeled as MA_1 seems slightly better than its counterpart Arima(0,1,2) labeled MA_1. The fit values are all a bit higher for the (0,1,2) process than the (0,1,1). 

### Testing against auto.arima

```{r}
auto.arima(csgo.ts)
```

Auto.arima() indicates an ARIMA(0,1,1) as its "best", this matches what I found in this case to be the best considering the structure of data and the performance of fit. Arima(0,1,1) can also be called "simple exponential smoothing" and the model equation can be written as $X_{t} = Y_{t-1} + \epsilon_{t} + 0.4896\epsilon_{t-1} $

### Forecast using training testing set

```{r}

csgo.train <- window(csgo.ts, end=c(2021,1))
csgo.test <- window(csgo.ts, start = c(2021,1))

train_fit <- Arima(order= c(0,1,1), csgo.train)

preds <- forecast(train_fit, h = 8)

autoplot(csgo.train) +
  autolayer(preds, series = "Arima(0,1,1) Prediction") +
  autolayer(csgo.test, series = "Actual", size = 2) + 
  labs(y = "Avg Monthly Players") + 
  scale_y_continuous(labels = scales::comma)

preds
csgo.test

```

Seems the model's capability to predict future values is somewhat validated looking at the results of the table and the plot. The true values are within the predicted intervals.

### Genreal forecasts 

```{r}

overall_preds <- forecast(MA1.csgo, h=12)

autoplot(csgo.ts) +
  autolayer(overall_preds, series ="Predictions") + 
  labs(y = "Avg Monthly Players")
  
overall_preds

```


## Game 4: Sid Meier's Civilization V

### Game Description & Data Processing

Sid Meier's Civilzation V, commonly called Civ V, is a 4x strategy game that is mostly played single player but can also be played multiplayer.

```{r}
steam <- read.csv("AllSteamData.csv")
civ5 <- steam %>%
  filter(Name == "Sid Meier's Civilization V") %>%
  filter(Avg..Players > 0)
civ5 <- civ5[rev(rownames(civ5)),]
civ5ts <- ts(civ5[,3], frequency = 12, start=c(2012, 7))
```

### ACF and PACF Plots

```{r}
autoplot(civ5ts)
ggAcf(civ5ts)
periodogram(civ5ts)
```

Our process is definitely not stationary, and our ACF plot definitely indicates some sort of AR or differenced process.

### Seasonality Analysis

```{r}
periodogram(civ5ts)
```

No initial seasonality is found in the data.

### Model Building

```{r}
fit0 <- Arima(civ5ts, order = c(1,0,0))
ggAcf(fit0$residuals)
fit0
```

An AR1 model leaves us with a value close to 1, so we differce the model. We also see a pattern in the residual ACF, so we check for seasonality.

```{r}
fit1 <- Arima(civ5ts, order = c(0,1,0))
periodogram(fit1$residuals)
abline(v=2/12)
```

We see a distinct line at 2/12, so we have a half year seasonal process. This is likely due to number of players increasing in the summer and winter due to time off school, and number of players decreasing in spring and fall due to school.

```{r}
fit2 <- Arima(civ5ts, order=c(0,1,0), seasonal=list(order=c(0,1,0), period=6))
ggAcf(fit2$residuals)
```

We see a spike at time 1 and 6, indicating a MA and seasonal MA component should be added to the model.

```{r}
fit3 <- Arima(civ5ts, order=c(0,1,1), seasonal=list(order=c(0,1,1), period=6))
ggAcf(fit3$residuals)
```

Our residuals look like white noise as is, but we will compare the current model to models with an added AR component to see if any perform better.

```{r}
fit4 <- Arima(civ5ts, order=c(1,1,1), seasonal=list(order=c(0,1,1), period=6))
fit5 <- Arima(civ5ts, order=c(1,1,1), seasonal=list(order=c(1,1,1), period=6))
fit6 <- Arima(civ5ts, order=c(0,1,1), seasonal=list(order=c(1,1,1), period=6))

getAIC <- function(fit) {
  b <- Box.test(fit$residuals, lag = 8, type = "Ljung")
  c(fit$aicc, b$p.value)
}

tab <- rbind(getAIC(fit1), getAIC(fit2), getAIC(fit3),
             getAIC(fit4), getAIC(fit6),getAIC(fit5))
colnames(tab) <- c("AICc", "Box Test p-value")
rownames(tab) <- c("I(1)","I(1), sI(1)[6]","IMA(1,1),sIMA(1,1)[6]","ARIMA(1,1,1),sIMA(1,1)[6]",
 "IMA(1,1),sARIMA(1,1,1)[6]","ARIMA(1,1,1),sARIMA(1,1,1)[6]")
kable(tab)
```

The 3rd model has the best p-value, but there are 3 other models with AICcs within 3, so we compare the model with the best AIC to the model with the best p-value

```{r}
#IMA(1,1) SIMA(1,1)[6]
tsdiag(fit3, gof.lag = 15, omit.initial=F)
#ARIMA(1,1,1) SARIMA(1,1,1)[6]
tsdiag(fit5, gof.lag = 15, omit.initial=F)
```

By comparing the output results, we find that the p-values for the smaller model are slightly worse, but still all significant. Becuase of this, we opt for the simpler model with the better AICc.

```{r}
fit3
autoplot(forecast(fit3, h=12)) +
  autolayer(fitted(fit3)) 
```

Our final model is

$y_{t} = y_{t-1}+y_{t-6}-y_{t-7}+\varepsilon_{t}-0.2138\varepsilon_{t-1}-0.732\varepsilon_{t-6} +0.1565 \varepsilon_{t-7}$

We see by forecasting our model into the future that we expect the average player base to steadily decrease over time.
